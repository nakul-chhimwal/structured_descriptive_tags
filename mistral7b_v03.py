# -*- coding: utf-8 -*-
"""mistral7b0.3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yAeeLBfRkcGjA8H3jXgZSbgME5z8f5as
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q transformers accelerate bitsandbytes

!pip install -q torch
!pip install -q pandas # pd.Timestamp
!pip install -q rarfile # used for .rar archive member listing
!pip install -q nest_asyncio
!pip install -q tqdm # For the progress bar in async functions

!pip install -q "unstructured[all-docs]" "unstructured-inference[cpu]"
!apt-get update -y -qq
!apt-get install -y -q poppler-utils tesseract-ocr libreoffice
!apt-get install -y -q unrar

# - poppler-utils: Enables Unstructured.io to process PDFs more effectively (e.g., better text extraction, image handling).
# - tesseract-ocr: Provides OCR capabilities for scanned documents or images within documents, used by Unstructured.io.
# - libreoffice: Enables Unstructured.io to handle older Microsoft Office formats (.doc, .xls, .ppt) by converting them internally.
# - unrar: The command-line utility required by the `rarfile` Python package to extract .rar archive contents.

from huggingface_hub import login
login()

#mistral new prompt
import os
import json
import uuid
import logging
import asyncio
import nest_asyncio

# --- Hugging Face specific imports ---
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# --- Unstructured.io Import ---
from unstructured.partition.auto import partition

# For nested asyncio loops in Colab/Jupyter environments
nest_asyncio.apply()

# --- Custom JSON Formatter & Logging Setup ---
class CustomJsonFormatter(logging.Formatter):
    STANDARD_LOG_RECORD_ATTRS = {'name','levelname','pathname','filename','module','lineno','funcName','created','asctime','msecs','relativeCreated','thread','threadName','process','processName','message','args','exc_info','exc_text','stack_info','levelno','msg'}
    def format(self, record):
        log_record_dict = {"timestamp": self.formatTime(record, self.datefmt),"level": record.levelname,"message": self.format_value_for_json(record.getMessage()),"source_file": self.format_value_for_json(record.filename),"source_lineno": self.format_value_for_json(record.lineno),"source_func": self.format_value_for_json(record.funcName),}
        for key, value in record.__dict__.items():
            if key not in self.STANDARD_LOG_RECORD_ATTRS and not key.startswith('_'): log_record_dict[key] = self.format_value_for_json(value)
        if record.exc_info: log_record_dict['exc_info'] = self.formatException(record.exc_info)
        return json.dumps(log_record_dict)
    def format_value_for_json(self, value):
        if isinstance(value, (int, float, bool, type(None), str)): return value
        elif isinstance(value, (list, tuple)): return [self.format_value_for_json(item) for item in value]
        elif isinstance(value, dict): return {str(k): self.format_value_for_json(v) for k, v in value.items()}
        else: return str(value)

log_file_path = '/content/filereading.log'
for handler in logging.root.handlers[:]: logging.root.removeHandler(handler)
logger = logging.getLogger(); logger.setLevel(logging.INFO)
file_handler = logging.FileHandler(log_file_path, mode='w')
custom_json_formatter = CustomJsonFormatter(datefmt='%Y-%m-%d %H:%M:%S')
file_handler.setFormatter(custom_json_formatter); logger.addHandler(file_handler)
logging.info = logger.info; logging.error = logger.error; logging.warning = logger.warning
logging.info("Logging setup complete.", extra={'log_config_status': 'success'})


# --- LLMTagger Class ---
class LLMTagger:
    def __init__(self, model_id="mistralai/Mistral-7B-Instruct-v0.3"):
        self.model_id = model_id
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None; self.tokenizer = None
        try:
            if self.device == "cuda":
                nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)
                self.model = AutoModelForCausalLM.from_pretrained(self.model_id, quantization_config=nf4_config, device_map="auto")
            else:
                self.model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float32, device_map="cpu")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
            if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
            logging.info(f"Mistral model '{model_id}' loaded successfully.")
        except Exception as e:
            logging.error(f"Failed to load Hugging Face model '{self.model_id}'.", extra={'exception': str(e)})

    async def tag_text(self, text_content: str, filename: str) -> dict:
        if self.model is None or self.tokenizer is None:
            return {}

        system_prompt = f"""
You are an expert document analysis system. Your task is to analyze the document's content and its filename to extract key information as a single, parsable JSON object.

- Use both the filename and the document text to determine the "document_type".
- If a value for a key is not found, you MUST include the key and use an empty string "" for text values or an empty list [] for list values.
- For Excel or CSV files, read and understand all sheets and columns, infer the purpose of the dataset based on column names and sample data, and generate a well-structured summary.

The categories to extract are:

- "document_type": The specific type of the document (e.g., "Report", "Lecture Notes", "Invoice", "Email", "Policy Document").
- "governmental_entity": The full name of any government department, ministry, or agency mentioned.
- "location": Specific geographic locations (cities, states, countries).
- "person": A list of full names of individuals mentioned in the document. Also include any email addresses, usernames, or other identifying markers that may help recognize individuals associated with the content.
- "organization": A list of full names of organizations, institutions, or companies mentioned (both governmental and non-governmental).
- "date": Key dates relevant to the document in YYYY-MM-DD format.
- "keywords": A list of 5–10 Notable fields or insights. keywords that are actually found within the document (e.g., column headers, section titles, repeated terms, and key content-related phrases).

- "contextual_keywords": A list of inferred or descriptive keywords based on the overall purpose and theme of the document.

- "summary": A concise, 2–3 line contextual summary of the document's purpose and insights. (Specifically For Excel or CSV files, include: High-level purpose, Analytical or technical description and Notable fields or insights.)

- "is_confidential": Return true if the document is implicitly or explicitly confidential. Evaluate based on the following indicators, not just the word 'confidential':
    - Explicit Markings: Contains phrases like 'For Internal Use Only', 'Restricted', 'Secret', 'Not for Distribution', or 'Proprietary Information'.
    - Sensitive Content: Contains Personally Identifiable Information (PII) like Aadhaar numbers, PAN cards, bank account details, passport numbers, or extensive lists of personal contact information.
    - Document Nature: The document_type is inherently sensitive, such as an internal memo, legal contract, financial report, security audit, or investigation file.
    - Context: Discusses sensitive government/organizational matters not intended for public release (e.g., policy drafts, internal security procedures).
    Otherwise, return false.

- "is_suspicious": Return true if the document seems potentially malicious, deceptive, or unusual. Evaluate based on these indicators:
    - Phishing Attempts: Urges the reader to click strange links, enter credentials, or contains threats (e.g., "your account will be suspended").
    - Unusual Content: Contains unexpected code snippets (like JavaScript, VBA macros), obfuscated URLs, or references to unofficial financial transactions.
    - Formatting: Appears intentionally garbled or contains unusual characters mixed with normal text to avoid detection.
    Otherwise, return false.

Ensure the output is ONLY the JSON object, without any explanatory text or markdown wrappers.
"""

        full_prompt_text = f"<s>[INST] <<SYS>>\n{system_prompt.strip()}\n<</SYS>>\n\nFilename: `{filename}`\n\nDocument Text:\n---\n{text_content}\n---[/INST]"
        try:
            inputs = self.tokenizer(full_prompt_text, return_tensors="pt", padding=True, truncation=True, max_length=16384).to(self.device)
            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=1024, do_sample=False, pad_token_id=self.tokenizer.pad_token_id, eos_token_id=self.tokenizer.eos_token_id)
            generated_text = self.tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()
            first_brace = generated_text.find('{'); last_brace = generated_text.rfind('}')
            if first_brace != -1 and last_brace > first_brace:
                return json.loads(generated_text[first_brace : last_brace + 1])
            else:
                raise ValueError("No valid JSON object found in model output.")
        except Exception as e:
            print(f"\n\nDEBUG: A CRITICAL ERROR occurred in tag_text: {e}\n")
            return {}


# --- Universal Document Text Extraction using Unstructured.io ---
async def extract_text_unstructured(file_path: str, processing_id: str) -> str:
    try:
        elements = await asyncio.to_thread(partition, filename=file_path, strategy="hi_res")
        return "\n\n".join([el.text for el in elements if hasattr(el, 'text')])
    except Exception as e:
        logging.error("Unstructured.io extraction failed.", extra={'processing_id': processing_id, 'source_document': os.path.basename(file_path), 'exception': str(e)})
        return ""


# --- Main Async Runner ---
async def run_dataset_extraction():
    #  CHANGE: Removed the 'if drive:' check for simplicity
    drive.mount('/content/drive')

    dataset_path = '/content/drive/MyDrive/xl/'
    unsupported_extensions = {'.gpkg', '.gdb', '.zip', '.rar', '.jpg', '.jpeg', '.png', '.gif'}

    all_files = [os.path.join(root, name) for root, _, files in os.walk(dataset_path) for name in files if not name.startswith('.')]

    print("\n--- Initializing Model ---")
    llm_tagger = LLMTagger()
    if llm_tagger.model is None:
        print("❌ LLM model could not be loaded. Aborting.")
        return
    print("--- Model Initialization Complete ---")

    from tqdm.asyncio import tqdm as async_tqdm
    for file_path in async_tqdm(all_files, desc="Processing Files"):
        processing_id = str(uuid.uuid4())
        file_name = os.path.basename(file_path)
        file_ext = os.path.splitext(file_name)[1].lower()

        if file_ext in unsupported_extensions:
            print(f"\n⏭️ Skipping unsupported/image format: {file_name}")
            continue

        extracted_content = await extract_text_unstructured(file_path, processing_id)
        llm_tags = {}

        if extracted_content:
            llm_tags = await llm_tagger.tag_text(extracted_content[:14000], file_name)
        else:
            logging.info(f"No content extracted from {file_name}. Generating default tags.",
                         extra={'id': processing_id, 'document': file_name})

        if not isinstance(llm_tags, dict):
            llm_tags = {}

        final_tags = {}
        final_tags['filename'] = file_name

        expected_keys = [
            "document_type", "governmental_entity", "location",
            "person_or_organization", "date", "keywords", "contextual_keywords", "summary",
            "is_confidential", "is_suspicious"
        ]

        for key in expected_keys:
            if key == "keywords":
                final_tags[key] = llm_tags.get(key, [])
            else:
                final_tags[key] = llm_tags.get(key, "")

        print(f"\n✅ Processed: {file_name}")
        print(f"   Extracted Tags: {json.dumps(final_tags, indent=2, ensure_ascii=False)}")
        logging.info("Successfully processed file.",
                     extra={'id': processing_id, 'document': file_name, 'tags': final_tags})


# --- Main Execution Block ---
if __name__ == "__main__":
    dataset_to_check = '/content/drive/MyDrive/xl/'
    if not os.path.exists(dataset_to_check):
        print(f"Error: Dataset directory '{dataset_to_check}' not found.")
    else:
        asyncio.run(run_dataset_extraction())
    print(f"\n--- Logs Saved ---")
    print(f"You can find the full JSON logs at: {log_file_path}")