# -*- coding: utf-8 -*-
"""idefics2.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VLvWpgJKv-Lh9iYIfLa1Wo4V8OCGA-D5
"""

!pip install -q -U transformers accelerate bitsandbytes tqdm

from huggingface_hub import login
login()

#new prompt
import torch
from transformers import Idefics2ForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, GenerationConfig
from PIL import Image, UnidentifiedImageError
import json
import os
import re
import gc
from google.colab import drive
from tqdm import tqdm

# Mount drive
print(" Mounting Google Drive...")
drive.mount('/content/drive', force_remount=True)
print(" Drive mounted. ")

#  Load IDEFICS2 Model and Processor
print("\n⏳ Loading IDEFICS2-8B model with 4-bit quantization...")
model_id = "HuggingFaceM4/idefics2-8b"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

processor = AutoProcessor.from_pretrained(model_id)
model = Idefics2ForConditionalGeneration.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto"
)

generation_config = GenerationConfig(
    max_new_tokens=1024,
    temperature=0.2,
    do_sample=True,
)
print("✅ IDEFICS2 model loaded successfully!")


# Define the Image Analysis Function
def analyze_image_idefics(image_path):
    try:
        image = Image.open(image_path).convert("RGB")

        prompt_instruction = """
Analyze the image and generate a single, valid JSON object based solely on the visual evidence provided.

If a value for a key is not found, you MUST include the key and use an empty string "" for text values or an empty list [] for list values.

The categories to extract are:
- "document_type": The specific type of the image. Infer if it is a "Photograph", "Screenshot", "Illustration", "ID Card", "Aadhaar Card", "PAN Card", etc.
- "governmental_entity": The full name of any government entity identified from text or symbols (flags, logos).
- "location": Only provide specific, identifiable geographic locations (e.g., a city, country, or famous landmark) if clearly visible or written. For generic scenes like 'park', 'river', or 'mountains', do not populate this field and return an empty string.
- "person": If specific names are not visible but people are present, describe the count of people, If no people are present, return an empty string.
- "organization": Only list names of companies or organizations if they are clearly identifiable in the image. Otherwise, return an empty string.
- "date": Only provide a date in YYYY-MM-DD format if it is clearly written in the image. Otherwise, return an empty string.
- "keywords": Provide an exhaustive list of every single object, item, and distinct visual feature you can detect in the image.
- "contextual_keywords": A list of inferred themes or concepts based on the overall scene (e.g., "travel", "office work", "celebration").
- "summary": A rich, detailed paragraph describing the entire scene, including the environment, interactions between objects, time of day, and overall mood or atmosphere.
- "is_confidential": Return true if the image contains sensitive information. Pay close attention to things that look like a 'Government Identification card', 'ID Card', 'Aadhaar Card', 'PAN Card', 'Passport', 'Credit Card', 'Cheques', 'bank statements' , visible passwords, or private documents and other things you think could be confidential. Otherwise, return false.
- "is_suspicious": Return true if the image depicts potentially dangerous, illegal, or gruesome content. Pay close attention to things like 'weapons', 'guns', 'knives', 'fighting', 'accidents', 'blood', or any signs of violence. Otherwise, return false.

Ensure the output is ONLY the JSON object.
"""
        conversation = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": prompt_instruction}]}]
        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
        inputs = processor(text=prompt, images=[image], return_tensors="pt").to("cuda")
        output = model.generate(**inputs, generation_config=generation_config)

        generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]
        assistant_response = generated_text.split("Assistant:")[1].strip()

        match = re.search(r'\{.*\}', assistant_response, re.DOTALL)
        if match:
            json_part = match.group(0)
            return json.loads(json_part)
        else:
            print(f"❗️Warning: Model did not return JSON for {os.path.basename(image_path)}.")
            return None

    except Exception as e:
        print(f"❗️IDEFICS2 error on {os.path.basename(image_path)}: {e}")
        return None

# Main Execution
if __name__ == "__main__":
    folder_path = "/content/drive/MyDrive/images/"
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}

    if not os.path.isdir(folder_path):
        print(f"❌ Error: The folder was not found at the path: {folder_path}")
    else:
        print(f"\n✅ Processing images from: {folder_path}\n")
        all_files = [f for f in os.listdir(folder_path) if not f.startswith('.')]

        for filename in tqdm(all_files, desc="Processing Images"):
            full_path = os.path.join(folder_path, filename)
            file_ext = os.path.splitext(filename)[1].lower()

            if file_ext not in image_extensions:
                print(f"--- Skipping non-image file: {filename} ---")
                continue

            print(f"\n--- Processing: {filename} ---")
            analysis_result = analyze_image_idefics(full_path)

            if analysis_result:
                print(json.dumps(analysis_result, indent=2))

            # Memory cleanup
            gc.collect()
            torch.cuda.empty_cache()